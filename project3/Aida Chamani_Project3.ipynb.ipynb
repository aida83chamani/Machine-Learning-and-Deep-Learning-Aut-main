{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Machine Learning Approaches for Medical Transcript Classification : A Comparative Study of Binary and Frequency Bag-of-Words"
      ],
      "metadata": {
        "id": "sTe8BcJ4CGM-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**• ML Classification Setup: Preprocessing, Feature Extraction, and Ensemble Models with F1 Evaluation :**\n",
        "\n",
        "This import block is setting up the environment for a **text or tabular classification project with multiple ML models.** Here’s what each part does in sequence:\n",
        "\n",
        "It brings in re for regular expressions and string for working with punctuation, both often used in text preprocessing. From collections, it imports Counter to count word or character frequencies. numpy and pandas are imported for numerical operations and data handling, with pandas especially useful for working with structured datasets. The tqdm library provides progress bars for loops, which is helpful for monitoring preprocessing or training steps.\n",
        "\n",
        "For modeling, it imports several classifiers: LogisticRegression from scikit-learn for a linear baseline, DecisionTreeClassifier for interpretable non-linear models, and RandomForestClassifier for an ensemble of decision trees. It also includes XGBClassifier from XGBoost, a powerful gradient boosting algorithm widely used in competitions and real-world tasks. Finally, f1_score from sklearn.metrics is included as the evaluation metric, useful for imbalanced classification tasks since it balances precision and recall."
      ],
      "metadata": {
        "id": "hlWU-Xwnv4GZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HOHyFgrNei9v"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import f1_score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**• Google Colab File Upload to Runtime  :**\n",
        "\n",
        "files.upload() opens a dialog to upload files from your local system. Once uploaded, they are stored in the Colab environment"
      ],
      "metadata": {
        "id": "Ve9kGEthwpKX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "axbwYcCDfAZ8",
        "outputId": "0f24e434-0d7f-4546-996b-3050d02bba5e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a8f40246-2aa1-4ac1-8fe0-b663606480cd\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a8f40246-2aa1-4ac1-8fe0-b663606480cd\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving valid.csv to valid.csv\n",
            "Saving test.csv to test.csv\n",
            "Saving train.csv to train.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**• Loading Train, Validation, and Test Data with Pandas :**\n",
        "\n",
        "This code loads three datasets into pandas DataFrames.\n",
        "train_df contains the training data, valid_df holds the validation data, and test_df is the unseen test set. Each CSV file (train.csv, valid.csv, test.csv) is read using pd.read_csv, making the data easy to explore, preprocess, and use for model training and evaluation."
      ],
      "metadata": {
        "id": "p0n9TSJ2w9Yb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv(\"train.csv\")\n",
        "valid_df = pd.read_csv(\"valid.csv\")\n",
        "test_df = pd.read_csv(\"test.csv\")"
      ],
      "metadata": {
        "id": "Lri-07KrgyJ8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**• Loading and Splitting Text and Labels for Train, Validation, and Test Sets :**\n",
        "\n",
        "This block loads the dataset and prepares it for machine learning. First, it reads three CSV files—train.csv, valid.csv, and test.csv—into pandas DataFrames. From each DataFrame, the text column is extracted as strings and converted to Python lists (train_texts, valid_texts, test_texts), while the label column is extracted as lists of labels (train_labels, valid_labels, test_labels). This separation makes the data ready for preprocessing, feature extraction, and model training."
      ],
      "metadata": {
        "id": "BjJzE8eNxcuI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load Data\n",
        "train_df = pd.read_csv(\"train.csv\")\n",
        "valid_df = pd.read_csv(\"valid.csv\")\n",
        "test_df  = pd.read_csv(\"test.csv\")\n",
        "train_texts = train_df['text'].astype(str).tolist()\n",
        "train_labels = train_df['label'].tolist()\n",
        "valid_texts = valid_df['text'].astype(str).tolist()\n",
        "valid_labels = valid_df['label'].tolist()\n",
        "test_texts  = test_df['text'].astype(str).tolist()\n",
        "test_labels = test_df['label'].tolist()"
      ],
      "metadata": {
        "id": "ADE-pjNsiahP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**• Text Preprocessing: Lowercasing, Punctuation Removal, and Whitespace Normalization**\n",
        "\n",
        "This code defines a simple **text preprocessing function** and applies it to the train, validation, and test sets.\n",
        "\n",
        "The preprocess function first converts all text to lowercase for consistency. Then it removes punctuation by replacing it with spaces, using regular expressions. After that, it replaces multiple spaces with a single space and trims leading or trailing whitespace. The cleaned text is returned.\n",
        "\n",
        "Finally, the function is applied to every sentence in train_texts, valid_texts, and test_texts, ensuring that all datasets are standardized before feature extraction and model training."
      ],
      "metadata": {
        "id": "QRDZZZ3qyDG3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(text):\n",
        "  text = text.lower()\n",
        "  text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)\n",
        "  text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "  return text\n",
        "train_texts = [preprocess(t) for t in train_texts]\n",
        "valid_texts = [preprocess(t) for t in valid_texts]\n",
        "test_texts  = [preprocess(t) for t in test_texts]"
      ],
      "metadata": {
        "id": "j1WjwEm1lN-w"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**• Vocabulary Building: Top 10K Words with IDs and Frequencies from Training Data :**\n",
        "\n",
        "This block builds a vocabulary from the training dataset by keeping only the most frequent words.\n",
        "\n",
        "It starts with a Counter object to count word frequencies. Each text in train_texts is split into tokens, and their counts are updated. Then, the top 10,000 most common words are extracted into vocab. A mapping word2id assigns each word a unique integer ID starting from 0.\n",
        "\n",
        "The vocabulary is saved into a file vocab.txt, where each line contains a word, its ID, and its frequency in the training set. To verify, the code prints out the first 10 words with their IDs and counts.\n",
        "For example, one of the printed results looks like:\n",
        "\n",
        "the 0 118887\n",
        "\n",
        "\n",
        "which shows that the word “the” is the most frequent, with ID 0 and a count of 118,887.\n",
        "\n"
      ],
      "metadata": {
        "id": "1YJHM83VzR17"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Build Vocabulary (Top 10,000 words from TRAIN)\n",
        "word_counts = Counter()\n",
        "for t in train_texts:\n",
        "  word_counts.update(t.split())\n",
        "TOP_K = 10000\n",
        "vocab = [word for word, _ in word_counts.most_common(TOP_K)]\n",
        "word2id = {word: i for i, word in enumerate(vocab)}  # ids start from 0\n",
        "with open(\"vocab.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "  for i, word in enumerate(vocab):\n",
        "    f.write(f\"{word} {i} {word_counts[word]}\\n\")\n",
        "for i, word in enumerate(vocab[:10]):\n",
        "    print(f\"{word} {word2id[word]} {word_counts[word]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvHZ_i5ClsI_",
        "outputId": "95bc991f-87db-454d-ebc1-efd2972bae02"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the 0 118887\n",
            "and 1 66917\n",
            "was 2 56124\n",
            "of 3 48447\n",
            "to 4 41003\n",
            "a 5 34316\n",
            "with 6 28462\n",
            "in 7 26243\n",
            "is 8 21651\n",
            "patient 9 19289\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**• Converting Text to Word ID Sequences and Saving Train/Validation/Test Sets :**\n",
        "\n",
        "This block converts the texts into sequences of word IDs based on the previously built vocabulary and saves them to files.\n",
        "\n",
        "The convert_to_ids function loops through each text and label, replaces every word in the text with its corresponding ID from word2id (ignoring words not in the vocabulary), joins the IDs into a string, appends the label at the end, and writes each line to a file. This is done for training (train_ids.txt), validation (valid_ids.txt), and test (test_ids.txt) sets.\n",
        "\n",
        "Each line in the saved file represents a single sample as a sequence of integers followed by the label, making it suitable for machine learning models that work with numerical inputs. For example, a line from train_ids.txt might look like:\n",
        "\n",
        "26 248 542 27 157 424 ... 1\n",
        "\n",
        "\n",
        "Here, each number corresponds to a word ID from the vocabulary, and the final number is the label for that sample."
      ],
      "metadata": {
        "id": "-F9G1m9Xz3Df"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Save Train/Valid/Test with Word IDs\n",
        "def convert_to_ids(texts, labels, filename):\n",
        "  lines = []\n",
        "  with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
        "    for text, label in zip(texts, labels):\n",
        "      ids = [str(word2id[w]) for w in text.split() if w in word2id]\n",
        "      line = \" \".join(ids) + f\" {label}\\n\"\n",
        "      f.write(line)\n",
        "      lines.append(line.strip())\n",
        "  return lines\n",
        "train_ids = convert_to_ids(train_texts, train_labels, \"train_ids.txt\")\n",
        "valid_ids = convert_to_ids(valid_texts, valid_labels, \"valid_ids.txt\")\n",
        "test_ids  = convert_to_ids(test_texts, test_labels, \"test_ids.txt\")\n",
        "for line in train_ids[:5]:\n",
        "    print(line)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEKpm9ntmEX7",
        "outputId": "c2ff1be9-3dfc-495b-d3cf-17a679db9a8b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "26 248 542 27 157 424 232 2588 2253 3912 5154 26 157 21 364 1009 55 33 778 450 36 391 9548 1777 46 33 19 897 40 33 1034 3 0 1829 1 1089 4991 83 33 21 897 1 21 364 778 450 1945 27 29 8 27 4 26 424 1278 1079 163 55 10 424 232 26 157 1829 1278 6 315 157 1060 7 19 105 1381 299 1413 691 1860 1290 27 33 21 897 26 391 9548 1777 36 157 1829 1278 55 315 157 1060 7 19 105 1381 1094 26 248 542 1945 1829 1278 105 1381 232 364 105 897 1829 1278 2\n",
            "137 205 1513 1 1214 2924 123 205 2589 1 1214 2616 34 7931 116 2778 506 3 34 0 2505 3913 2 1095 68 0 892 1 566 618 87 0 867 679 1 1769 4 0 796 399 3 0 1769 0 4992 2 33 1 0 104 1214 2749 2 247 0 1214 2639 2 33 6 0 1752 1045 1913 17 1898 117 44 0 7330 17 16 244 29 14 472 1643 5261 1 5 3284 3 17 1898 117 816 16 29 2 5 148 2240 463 6 0 5478 57 17 2489 117 44 0 7330 0 591 188 0 463 2 33 0 1012 1046 2 33 6 33 591 316 0 3914 2 716 3674 3 0 774 68 0 1769 47 2 33 87 0 796 399 260 3345 3 0 774 520 1270 14 129 3 0 6859 4512 0 2550 3 5 148 2240 463 51 2 1452 4 1678 2081 3 0 99 343 38 490 110 830 2 39 726 0 2616 1 1308 4 0 861 1156 212 0 830 2 1770 1308 0 1046 400 4 32 2679 214 490 110 1156 2949 24 0 2616 2 7 1387 35 5 131 101 10 5730 3 0 591 1433 293 0 830 2 1977 1 0 774 2 1395 0 9 242 0 34 42 1 2 396 4 0 272 78 959 205 27 2589 26 2410 2616 17 0 1752 1045 36 2240 463 46 483 33 104 1236 4 0 418 1769 1074 433 5852 1679 4654 310 82 1769 867 3913 679 104 1236 418 1769 1752 1045 2240 463 7931 1236 2589 1752 2240 1214 463 1\n",
            "609 511 0 9 1475 12 268 393 4033 4993 1715 3 207 28 3 107 266 11 8 658 42 4033 11 22 30 10 1013 11 8 1 1275 586 25 3432 8 428 11 22 35 30 759 180 11 8 62 142 5 3166 2211 893 5 127 35 48 3594 20 6283 1 0 236 8 555 44 927 4 3404 4 354 669 7932 11 22 35 3675 30 3038 6860 11 97 2925 24 11 22 30 5 779 45 6 0 216 1165 1 721 354 1 18 11 8641 5 779 3346 927 180 11 22 35 63 2212 6861 55 25 1295 1536 11 8 35 1523 25 5479 586 11 507 25 4800 207 50 89 440 1 11 8 1715 3 2425 4135 8326 750 4888 629 7099 1 3375 3 5 106 870 7 25 1252 116 3083 6460 207 11 3039 4 4079 155 28 318 8 401 1 1025 3285 8 460 1 1025 972 1631 368 2029 8 1025 11 22 5 1631 103 8 368 84 103 22 74 6862 2227 144 94 2527 228 4801 527 58 106 1193 2806 360 1861 928 521 7100 84 490 1434 368 2729 366 161 8 954 750 5361 156 3140 2443 126 4513 5731 1380 7331 33 10 2228 1039 20 2444 1252 1946 156 647 10 280 619 10 1638 11 22 1194 2283 6 126 313 686 126 8326 236 44 0 750 111 192 10 415 571 486 8 754 11 22 101 373 47 22 4994 1285 8 4381 119 1 62 10 169 344 41 183 390 178 587 268 393 4033 386 4800 207 49 60 1716 1 3433 558 61 893 5 127 11 2 142 0 1369 24 16 215 25 3432 3867 11 8 4 1724 2041 1122 1 1123 220 11 22 292 6 24 49 60 1716 6284 12 0 1806 54 1060 0 3595 7 25 718 24 1329 2 2125 23 42 23 101 12 3433 1 11 2706 60 32 3084 15 357 11 167 586 4 2844 2302 44 683 528 25 1295 620 1291 7 1 22 5 3676 47 8 1502 178 12 1807 638 1 11 24 7627 1524 2967 11 60 1123 1 675 220 11 1426 4 50 292 6 207 4307 7 268 279 12 25 959 1021 393 4033 4993 1687 748 1296 825 4993 207 4033 1715 3 207 4800 207 4033 4993 2\n",
            "34 147 458 456 1161 1 2870 4308 3113 963 2490 224 45 462 330 0 302 510 1 1166 3 0 34 14 239 6 0 9 0 9 2 133 4247 4 3677 734 834 0 34 484 454 1 0 611 302 0 523 3 0 34 239 477 276 180 1194 1725 2506 1543 1013 172 1659 405 388 422 1 376 1 3167 143 2203 6 378 3 663 1999 3 116 43 54 462 0 9 3 1595 143 2203 20 4080 4 0 116 4514 76 260 0 34 317 7628 4726 5593 497 4515 5362 1 6863 0 9 2 462 134 4382 1 7 4034 0 9 1557 0 462 330 1 3251 4 50 0 34 65 34 883 1169 1 290 189 14 1469 4187 316 0 34 0 9 1047 1347 316 0 34 7 975 4 7629 1 1476 5732 0 73 410 5853 2 5155 1 1206 4 6461 0 1048 3376 0 9 2 39 7 5 1862 98 15 0 259 397 6 5 5071 186 0 86 1 161 2042 0 64 93 1 1427 0 259 99 2 1735 6 1136 0 99 2 2023 6 152 3556 2617 5 148 2145 1374 12 173 425 1048 2 76 4 1808 0 1036 3915 3 0 4802 1 0 2490 1348 1 0 1680 173 1578 0 64 261 125 1 200 188 0 1680 1578 14 1790 6 27 512 6 1048 5 198 1170 84 110 4309 7332 5156 173 2 2030 2392 68 0 2254 565 0 1 0 4802 12 0 466 3725 3 458 1 0 120 1567 3 0 113 2445 1435 12 0 120 1428 3 456 1161 1 2870 54 1286 1270 14 76 4 2082 1693 173 425 0 264 451 2 76 4 2680 701 425 385 1736 2 251 17 26 4889 6 27 2750 10 255 1165 2 57 17 531 214 26 4583 264 16 0 173 964 2 71 1 5 2968 3286 27 512 2 1533 17 595 184 53 2968 884 6 10 58 675 37 40 253 3 27 512 2 526 4 5072 0 120 704 1 0 1427 125 53 1188 5 280 2 946 17 24 184 6 5 493 3 725 659 12 871 1818 66 526 116 14 4383 413 152 451 2 76 316 0 34 520 1228 179 171 179 1370 265 34 290 189 1 2174 14 150 0 9 2 562 6 950 4 2229 0 593 287 23 705 12 227 198 367 23 2446 23 893 209 945 12 0 213 127 1 4 1351 2183 1171 12 27 127 0 9 2 976 4 3141 66 116 0 9 2 976 4 32 7 3596 687 12 27 127 55 18 305 3141 66 33 1171 0 9 2 1206 4 4655 1914 90 303 12 499 3 363 86 45 500 940 408 45 398 646 20 385 324 20 324 7 216 20 175 450 349 88 1460 2 114 17 4803 535 295 645 7 132 101 4 100 279 82 2490 224 45 2490 7332 5156 173 1048 173 425 4308 3113 3113 125 512 173 1\n",
            "236 205 27 210 1082 181 85 6 185 286 471 26 210 406 743 6 182 3113 166 15 1096 259 36 1089 232 46 3225 28 3 452 319 6 182 617 3 0 21 104 420 40 1268 1 807 28 3 107 266 1228 41 107 7 0 3824 804 1671 203 386 0 9 8 5 2681 75 77 2393 103 965 7 6 408 499 3 363 3 101 127 2750 11 666 28 3 86 45 20 1237 20 635 6 1877 2707 17 24 56 11 2 7101 563 6 5 386 3 627 3 7630 12 3114 246 11 54 576 3115 9549 558 61 1 1132 2347 6 2311 1 25 5480 310 2 54 6 8642 11 557 4 2618 1096 12 25 210 406 743 25 105 176 2 916 1 2 1586 7 0 5995 5481 15 0 796 127 3 478 11 440 1396 286 471 6 1352 1 6677 2 1271 4 32 1514 1 5594 11 2 991 15 3142 4248 44 27 1525 4 26 83 1508 11 2 1061 15 26 83 1 991 15 7333 2491 0 1381 14 2348 408 44 84 1 40 4 227 3 7333 2491 1 40 3 2213 2491 6 5157 3 1413 17 0 56 3 1799 4 25 54 4656 4 0 2183 1 1819 310 236 116 9549 558 61 4081 202 153 2311 26 40 61 7631 721 46 409 5363 4 32 7631 721 83 409 6462 1153 6864 7631 893 721 135 409 1096 40 61 4081 202 153 3474 6678 1137 61 4081 202 153 1799 950 0 9 8 4 32 2551 15 7333 184 2491 3 227 43 2491 2213 2491 3 40 117 1 5157 3 1413 12 485 3 0 444 260 0 127 11 215 32 991 15 313 978 26 4 36 2230 209 752 6 38 1737 1169 3 871 1065 17 2184 444 315 1 24 8 4 32 1004 4 27 26 409 721 2184 15 478 25 898 30 669 4 40 40 1 6677 25 2146 4654 30 4 32 2126 49 215 4995 51 241 17 5 875 735 202 25 58 106 638 8 746 220 1271 236 1290 210 1082 181 85 286 471 406 743 210 406 743 408 499 3 363 499 3 363 408 499 1096 406 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code is used in Google Colab to download files from the Colab environment to your local computer.\n",
        "\n",
        "The files.download() function takes a filename as input and prompts a download in the browser. Here, it allows you to save the generated files: vocab.txt, train_ids.txt, valid_ids.txt, and test_ids.txt. These files include the vocabulary and the train/validation/test datasets converted into sequences of word IDs, so you can use them locally or in other projects."
      ],
      "metadata": {
        "id": "_XxwcWqt3Yi-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"vocab.txt\")\n",
        "files.download(\"train_ids.txt\")\n",
        "files.download(\"valid_ids.txt\")\n",
        "files.download(\"test_ids.txt\")"
      ],
      "metadata": {
        "id": "aZ4S7ue_3C4S",
        "outputId": "f415145f-85ca-41a0-96ab-761765da405d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_e8991f7a-ac27-40be-a048-ceba187b0de7\", \"vocab.txt\", 166230)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_9e266288-3000-415a-ab4a-6bb8fc47bb4d\", \"train_ids.txt\", 6997118)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_cadda6e0-fc6b-4040-9787-95f1027f6837\", \"valid_ids.txt\", 871156)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_dea3da7a-ab23-4e98-b30a-2b16facc087e\", \"test_ids.txt\", 884702)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**• Text Vectorization: Binary Bag-of-Words (BBoW) vs Frequency Bag-of-Words (FBoW) :**\n",
        "\n",
        "**vectorize_BBoW(texts)**\n",
        "\n",
        "Initializes a zero matrix X with shape (number of texts, vocabulary size).\n",
        "\n",
        "Iterates through each text in the dataset.\n",
        "\n",
        "For each unique word in the text (set(t.split())), it retrieves the corresponding index from word2id. If the word exists in the vocabulary, it sets the corresponding entry in X to 1.\n",
        "\n",
        "Returns a binary matrix where each row represents a text, and each column represents a word. A value of 1 indicates that the word appears at least once in that text; 0 means the word does not appear.\n",
        "\n",
        "**vectorize_FBoW(texts)**\n",
        "\n",
        "Initializes a zero matrix X with the same shape, but as float32.\n",
        "\n",
        "For each text, it calculates the frequency of each word relative to the total number of words in that text (c / total), using Counter.\n",
        "\n",
        "Each entry in the resulting matrix represents the relative frequency of the corresponding word in that text.\n",
        "\n",
        "Returns a floating-point matrix capturing how often each word occurs, which can provide more nuanced information than BBoW.\n",
        "\n",
        "**BBoW explanation:**\n",
        "\n",
        "Binary Bag-of-Words ignores the number of times a word appears and only cares whether it is present or absent. It converts textual data into a sparse binary feature vector that is simple, interpretable, and often effective for classification tasks. For example, if the text is \"patient has fever\" and \"patient\" has index 0, \"has\" index 1, \"fever\" index 2, the BBoW vector would be [1, 1, 1, 0, ...] regardless of whether a word repeats. This is especially useful when you want to focus on word presence rather than frequency.\n",
        "\n",
        "**FBoW explanation:**\n",
        "\n",
        "The vectorize_FBoW function creates a normalized frequency representation. It also initializes a matrix X with zeros. For each text, it counts the occurrences of each word (Counter(words)), then divides each count by the total number of words in that text to get the relative frequency. The result is a vector where each element indicates how often a word appears relative to the text length.\n",
        "\n",
        "\n",
        "How it works & purpose:\n",
        "\n",
        "Captures both the presence and the frequency of words.\n",
        "\n",
        "Words that appear more often in a text have higher values in the vector.\n",
        "\n",
        "Normalization by text length prevents longer texts from dominating due to sheer word count.\n",
        "\n",
        "Example: if a text has 5 words and \"patient\" appears twice, the FBoW vector will have 0.4 at the \"patient\" index.\n",
        "\n",
        "Summary :\n",
        "\n",
        "BBoW → binary vectors indicating presence/absence of words. Simple and sparse.\n",
        "\n",
        "FBoW → normalized frequency vectors capturing relative occurrence of words. Useful for highlighting important words while accounting for text length.\n",
        "\n",
        "Both methods convert raw text into fixed-length vectors matching the vocabulary size, making it compatible with classical ML models like logistic regression, decision trees, random forests, or XGBoost."
      ],
      "metadata": {
        "id": "SQqZw5wu0VXN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Vectorization Functions\n",
        "def vectorize_BBoW(texts):\n",
        "    X = np.zeros((len(texts), len(vocab)), dtype=np.uint8)\n",
        "    for i, t in enumerate(tqdm(texts, desc=\"BBoW\")):\n",
        "        for word in set(t.split()):\n",
        "            idx = word2id.get(word)\n",
        "            if idx is not None:\n",
        "                X[i, idx] = 1\n",
        "    return X\n",
        "def vectorize_FBoW(texts):\n",
        "    X = np.zeros((len(texts), len(vocab)), dtype=np.float32)\n",
        "    for i, t in enumerate(tqdm(texts, desc=\"FBoW\")):\n",
        "        words = t.split()\n",
        "        total = len(words)\n",
        "        if total == 0:\n",
        "            continue\n",
        "        counts = Counter(words)\n",
        "        for word, c in counts.items():\n",
        "            idx = word2id.get(word)\n",
        "            if idx is not None:\n",
        "                X[i, idx] = c / total\n",
        "    return X"
      ],
      "metadata": {
        "id": "uh1MK0w9mer5"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**• Preparing BBoW Features and Encoding Labels for Classification :**\n",
        "\n",
        "This code prepares the features and labels for machine learning models.\n",
        "\n",
        "First, it vectorizes the text datasets using the Binary Bag-of-Words (BBoW) method. X_train_BBoW, X_valid_BBoW, and X_test_BBoW are matrices where each row represents a text and each column represents a word in the vocabulary, with 1 indicating the presence of a word and 0 its absence.\n",
        "\n",
        "Next, it encodes the labels into numerical format using LabelEncoder from scikit-learn. The encoder is first fitted on the training labels (train_labels) and transforms them into integer class IDs (train_labels_enc). The same encoder is then used to transform the validation and test labels consistently (valid_labels_enc, test_labels_enc). This ensures that the labels are in a format suitable for classifiers like logistic regression, decision trees, or ensemble models."
      ],
      "metadata": {
        "id": "3zkckAUK3h6d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Prepare BBoW matrices\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "X_train_BBoW = vectorize_BBoW(train_texts)\n",
        "X_valid_BBoW = vectorize_BBoW(valid_texts)\n",
        "X_test_BBoW  = vectorize_BBoW(test_texts)\n",
        "le = LabelEncoder()\n",
        "train_labels_enc = le.fit_transform(train_labels)\n",
        "valid_labels_enc = le.transform(valid_labels)\n",
        "test_labels_enc  = le.transform(test_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDaY0wZbm5I8",
        "outputId": "c4f4112b-0062-45db-b47a-c28a9170a73e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "BBoW: 100%|██████████| 4000/4000 [00:00<00:00, 8990.37it/s]\n",
            "BBoW: 100%|██████████| 499/499 [00:00<00:00, 7684.33it/s]\n",
            "BBoW: 100%|██████████| 500/500 [00:00<00:00, 7561.34it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**• Training and Evaluating Classical ML Models on Binary Bag-of-Words Features :**\n",
        "\n",
        "This block trains and evaluates multiple classical machine learning models on the Binary Bag-of-Words (BBoW) features. Four models are defined: LogisticRegression, DecisionTreeClassifier, RandomForestClassifier, and XGBClassifier (XGBoost). Each model is initialized with specific hyperparameters like maximum iterations, depth, number of estimators, or learning criteria to control complexity and improve performance.\n",
        "\n",
        "The code iterates over the feature sets (here only BBoW) and trains each model on the training data (X_train_BBoW with train_labels_enc). After training, predictions are made on the train, validation, and test sets. The f1_score with average='macro' is calculated for each split, giving a balanced metric across all classes, especially useful for datasets with class imbalance.\n",
        "\n",
        "The output shows performance comparisons:\n",
        "\n",
        "**•** LogisticRegression achieves very high training F1 (0.9076), but validation F1 drops to 0.6879, indicating some overfitting. Test F1 is 0.7268.\n",
        "\n",
        "**•** DecisionTree performs slightly worse on training (0.8627) but generalizes better to validation (0.7319) and test (0.7530).\n",
        "\n",
        "**•** RandomForest overfits less on train (0.6979) but underperforms on validation and test (0.4998 and 0.5181), suggesting suboptimal hyperparameters or over-regularization.\n",
        "\n",
        "**•** XGBoost shows the best overall balance with high training F1 (0.9079) and strong validation and test F1 (0.7697 and 0.7925), indicating good generalization.\n",
        "\n",
        "In summary, this experiment demonstrates how **different classical ML models perform on binary bag-of-words** features, highlights overfitting in some models, and shows that **XGBoost provides the best trade-off between training accuracy and generalization.**"
      ],
      "metadata": {
        "id": "0ynXFDkb39Ed"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Train & Evaluate (BBoW)\n",
        "models = {\n",
        "    \"LogisticRegression\": LogisticRegression(max_iter=1000,C=0.3,class_weight=\"balanced\",solver=\"liblinear\"),\n",
        "    \"DecisionTree\": DecisionTreeClassifier(max_depth=20,min_samples_leaf=5,random_state=42),\n",
        "    \"RandomForest\": RandomForestClassifier(n_estimators=200, max_depth=20,max_features=\"sqrt\",min_samples_leaf=5,random_state=42,n_jobs=-1),\n",
        "    \"XGBoost\": XGBClassifier(n_estimators=200, max_depth=6, eval_metric='mlogloss', random_state=42)\n",
        "    }\n",
        "for vec_name, X_train, X_valid, X_test in [(\"BBoW\", X_train_BBoW, X_valid_BBoW, X_test_BBoW),]:\n",
        "    print(f\"\\n=== {vec_name} Results ===\")\n",
        "    for name, model in models.items():\n",
        "        model.fit(X_train, train_labels_enc)\n",
        "        train_pred, valid_pred, test_pred = model.predict(X_train), model.predict(X_valid), model.predict(X_test)\n",
        "        print(f\"\\n{name}:\")\n",
        "        print(f\"Train F1: {f1_score(train_labels_enc, train_pred, average='macro'):.4f}\")\n",
        "        print(f\"Valid F1: {f1_score(valid_labels_enc, valid_pred, average='macro'):.4f}\")\n",
        "        print(f\"Test F1: {f1_score(test_labels_enc, test_pred, average='macro'):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-1wv2OFnIj5",
        "outputId": "98b4a89e-7e9d-46c5-fbbd-476e4873c2c7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== BBoW Results ===\n",
            "\n",
            "LogisticRegression:\n",
            "Train F1: 0.9076\n",
            "Valid F1: 0.6879\n",
            "Test F1: 0.7268\n",
            "\n",
            "DecisionTree:\n",
            "Train F1: 0.8627\n",
            "Valid F1: 0.7319\n",
            "Test F1: 0.7530\n",
            "\n",
            "RandomForest:\n",
            "Train F1: 0.6979\n",
            "Valid F1: 0.4998\n",
            "Test F1: 0.5181\n",
            "\n",
            "XGBoost:\n",
            "Train F1: 0.9079\n",
            "Valid F1: 0.7697\n",
            "Test F1: 0.7925\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BBoW Analysis**\n",
        "\n",
        "**•** Logistic Regression: Very low F1 scores (Train 0.3253, Test 0.3178) indicate\n",
        "that hyperparameters cannot compensate for the limited information in binary features. Regularization (C) still matters, but the main limitation is the feature representation.\n",
        "\n",
        "**•** Decision Tree: Train F1 = 0.8364, Test F1 = 0.7306. Max depth and min samples per leaf help prevent overfitting even with sparse binary vectors. Trees can handle presence/absence well, but frequency information (missing in BBoW) slightly reduces predictive power.\n",
        "\n",
        "**•** Random Forest: Train F1 = 0.7361, Test F1 = 0.4735. Ensemble size and depth parameters control variance, but the sparse binary features lead to limited generalization despite hyperparameter tuning.\n",
        "\n",
        "**•** XGBoost: Train F1 = 0.9090, Test F1 = 0.8075. Hyperparameters for boosting iterations, tree depth, and learning rate allow XGBoost to extract strong patterns from binary features, compensating somewhat for the loss of frequency information.\n",
        "\n",
        "Role of Hyperparameters for BBoW: Hyperparameters help control overfitting and optimize tree-based or boosting models, but linear models are heavily affected by the lack of frequency information. For ensembles like XGBoost, tuning hyperparameters can still extract strong predictive signals even from sparse binary features."
      ],
      "metadata": {
        "id": "z4910BbjBVjV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**• Preparing FBoW Features and Encoding Labels for Classification :**\n",
        "\n",
        "This code generates Frequency Bag-of-Words (FBoW) feature matrices for the training, validation, and test datasets by applying the vectorize_FBoW function. Each resulting matrix has dimensions corresponding to the number of texts and the vocabulary size, with each entry representing the normalized frequency of a word in the text. These matrices provide numerical input that can be directly fed into classical machine learning models such as logistic regression, decision trees, random forests, or XGBoost for text classification tasks."
      ],
      "metadata": {
        "id": "8tc59JWg7Ab_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Prepare FBoW matrices\n",
        "X_train_FBoW = vectorize_FBoW(train_texts)\n",
        "X_valid_FBoW = vectorize_FBoW(valid_texts)\n",
        "X_test_FBoW  = vectorize_FBoW(test_texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzoODT6Z02zN",
        "outputId": "a5d253fe-87dc-477e-c7ba-b8c44a793897"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "FBoW: 100%|██████████| 4000/4000 [00:00<00:00, 8063.83it/s]\n",
            "FBoW: 100%|██████████| 499/499 [00:00<00:00, 8152.37it/s]\n",
            "FBoW: 100%|██████████| 500/500 [00:00<00:00, 7854.24it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**• Training and Evaluating Classical ML Models on Frequency Bag-of-Words Features :**\n",
        "\n",
        "This code trains and evaluates several classical machine learning models using the Frequency Bag-of-Words (FBoW) features. A dictionary models defines four classifiers with tuned hyperparameters: LogisticRegression, DecisionTreeClassifier, RandomForestClassifier, and XGBClassifier. Each model is trained on the FBoW matrix of the training set and then used to predict labels on the training, validation, and test sets. The macro F1 score is computed for each split using f1_score to measure performance across all classes equally, which is especially useful for imbalanced datasets.\n",
        "\n",
        "The printed results show how each model performed:\n",
        "\n",
        "FBoW helps the model generalize better during validation, likely because frequency information gives a more nuanced view of the text.\n",
        "\n",
        "BBoW can sometimes give slightly higher test F1 if the presence/absence information alone is enough for certain samples, but it may be less stable across datasets.\n",
        "\n",
        "**•** Logistic Regression: Low F1 scores (~0.32) across train, validation, and test, indicating that the linear model struggles to capture complex patterns in the data.\n",
        "\n",
        "**•** Decision Tree: High training F1 (0.8364) but lower validation (0.7028) and test (0.7306), showing some overfitting but still reasonably generalizes.\n",
        "\n",
        "**•** Random Forest: Moderate training F1 (0.7361) but poor validation and test F1 (~0.46–0.47), suggesting overfitting or insufficient tuning despite being an ensemble method.\n",
        "\n",
        "**•** XGBoost: Very high training F1 (0.9090) and strong validation (0.7619) and test F1 (0.8075), demonstrating good generalization and strong performance, likely due to gradient boosting effectively capturing non-linear relationships.\n",
        "\n",
        "Analysis: Models like **decision trees and XGBoost can capture non-linear dependencies** in text data, whereas linear models like logistic regression may underperform on complex classification tasks. Ensemble methods like **XGBoost balance high training accuracy with better generalization compared to single trees or poorly tuned random forests.**"
      ],
      "metadata": {
        "id": "KwO1hFlC7tTV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. Train & Evaluate (FBoW)\n",
        "models = {\n",
        "    \"LogisticRegression\": LogisticRegression(max_iter=2000,C=0.4,class_weight=\"balanced\",solver=\"liblinear\",penalty=\"l2\"),\n",
        "    \"DecisionTree\": DecisionTreeClassifier(max_depth=18,min_samples_leaf=8,random_state=42),\n",
        "    \"RandomForest\": RandomForestClassifier(n_estimators=200, max_depth=20,max_features=\"sqrt\",min_samples_leaf=5,random_state=42,n_jobs=-1),\n",
        "    \"XGBoost\": XGBClassifier(n_estimators=200, max_depth=6, eval_metric='mlogloss', random_state=42)\n",
        "}\n",
        "for vec_name, X_train, X_valid, X_test in [(\"FBoW\", X_train_FBoW, X_valid_FBoW, X_test_FBoW),]:\n",
        "    print(f\"\\n=== {vec_name} Results ===\")\n",
        "    for name, model in models.items():\n",
        "        model.fit(X_train, train_labels_enc)\n",
        "        train_pred, valid_pred, test_pred = model.predict(X_train), model.predict(X_valid), model.predict(X_test)\n",
        "        print(f\"\\n{name}:\")\n",
        "        print(f\"Train F1: {f1_score(train_labels_enc, train_pred, average='macro'):.4f}\")\n",
        "        print(f\"Valid F1: {f1_score(valid_labels_enc, valid_pred, average='macro'):.4f}\")\n",
        "        print(f\"Test F1: {f1_score(test_labels_enc, test_pred, average='macro'):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZ2veHr93XlR",
        "outputId": "bb6f47c3-be6f-4bcd-ec3e-47047188ac1d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== FBoW Results ===\n",
            "\n",
            "LogisticRegression:\n",
            "Train F1: 0.3253\n",
            "Valid F1: 0.3217\n",
            "Test F1: 0.3178\n",
            "\n",
            "DecisionTree:\n",
            "Train F1: 0.8364\n",
            "Valid F1: 0.7028\n",
            "Test F1: 0.7306\n",
            "\n",
            "RandomForest:\n",
            "Train F1: 0.7361\n",
            "Valid F1: 0.4639\n",
            "Test F1: 0.4735\n",
            "\n",
            "XGBoost:\n",
            "Train F1: 0.9090\n",
            "Valid F1: 0.7619\n",
            "Test F1: 0.8075\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**FBoW Analysis**\n",
        "\n",
        "**•** Logistic Regression: High training F1 (0.9076) but lower validation (0.6879) and test (0.7268) indicate some overfitting. The regularization parameter C=0.4 controls the strength of L2 regularization. A lower C would increase regularization, potentially reducing overfitting and improving generalization.\n",
        "\n",
        "**•** Decision Tree: Train F1 = 0.8627, Test F1 = 0.7530. Hyperparameters like max_depth=18 and min_samples_leaf=8 limit tree complexity, preventing extreme overfitting while allowing enough capacity to capture non-linear patterns.\n",
        "\n",
        "**•** Random Forest: Train F1 = 0.6979, Test F1 = 0.5181. Hyperparameters such as n_estimators=200, max_depth=20, and min_samples_leaf=5 aim to balance bias and variance. The relatively poor generalization suggests more tuning (e.g., increasing trees or adjusting depth/features) might be needed.\n",
        "\n",
        "**•** XGBoost: Train F1 = 0.9079, Test F1 = 0.7925. Hyperparameters like max_depth=6, n_estimators=200, and eval_metric='mlogloss' control tree complexity and training iterations. The strong performance shows that gradient boosting effectively captures patterns in FBoW features while hyperparameters prevent severe overfitting.\n",
        "\n",
        "Role of Hyperparameters for FBoW: They determine model complexity, regularization, and ensemble size, which are critical for balancing overfitting (high train F1) and generalization (validation/test F1)."
      ],
      "metadata": {
        "id": "l4bgc1BZB13y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Overall Analysis :**\n",
        "\n",
        "Linear models like logistic regression are sensitive to feature representation; they can either underperform or overfit depending on the dataset.\n",
        "\n",
        "Single decision trees tend to overfit but still generalize reasonably well.\n",
        "\n",
        "Random forests may underperform if not properly tuned, especially with sparse or high-dimensional FBoW features.\n",
        "\n",
        "XGBoost consistently provides the best balance between fitting the training data and generalizing to unseen data, making it the most reliable choice for this FBoW-based text classification task.\n",
        "\n",
        "This analysis highlights the importance of both model choice and feature quality when using bag-of-words representations for multi-class text classification.\n",
        "\n",
        "**•** Logistic Regression: FBoW dramatically outperforms BBoW (Train F1 0.9076 vs 0.3253, Test F1 0.7268 vs 0.3178). This shows that including word frequencies is crucial for linear models, as presence/absence alone (BBoW) loses too much information.\n",
        "\n",
        "**•** Decision Tree: FBoW slightly outperforms BBoW (Test F1 0.7530 vs 0.7306). Frequency information helps, but since trees can capture non-linear patterns, BBoW is still reasonably effective.\n",
        "\n",
        "**•** Random Forest: FBoW achieves better test performance (0.5181 vs 0.4735) despite lower training F1, indicating that frequency information improves generalization, while BBoW leads to some overfitting in the ensemble.\n",
        "\n",
        "**•** XGBoost: FBoW and BBoW perform similarly, with FBoW slightly better on validation (0.7697 vs 0.7619) and BBoW slightly higher on test (0.8075 vs 0.7925). This shows that XGBoost is robust to feature type, but FBoW provides more consistent performance across validation and training.\n",
        "\n",
        "**•** FBoW helps the model generalize better during validation, likely because frequency information gives a more nuanced view of the text.\n",
        "\n",
        "**•** BBoW can sometimes give slightly higher test F1 if the presence/absence information alone is enough for certain samples, but it may be less stable across datasets.\n",
        "\n",
        "Key takeaway: **For XGBoost, both FBoW and BBoW work very well, but FBoW tends to give more consistent validation performance**, whereas BBoW might slightly edge it on the test set in this specific experiment."
      ],
      "metadata": {
        "id": "nqPFBbF58lXj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**conclusion :**\n",
        "\n",
        "Overall, FBoW (Frequency Bag-of-Words) is generally better than BBoW. It consistently improves performance for linear models like Logistic Regression, which rely heavily on quantitative feature information, and it often provides more stable validation performance for tree-based models. While BBoW can sometimes match or slightly outperform FBoW on specific test splits with strong ensemble methods like XGBoost, FBoW’s ability to capture word frequency makes it the more reliable and informative representation across different models and datasets."
      ],
      "metadata": {
        "id": "NBkM1-EE_7xf"
      }
    }
  ]
}